# -*- coding: utf-8 -*-
import abc
import traceback
from typing import Any, Optional

import pandas as pd
from sklearn.preprocessing import StandardScaler

from ts_benchmark.data.data_pool import DataPool
from ts_benchmark.evaluation.strategy.constants import FieldNames
from ts_benchmark.evaluation.strategy.strategy import Strategy
from ts_benchmark.models import ModelFactory
from ts_benchmark.utils.data_processing import split_time
from ts_benchmark.utils.random_utils import fix_random_seed, fix_all_random_seed
import logging
import optuna

logger = logging.getLogger(__name__)

class ForecastingStrategy(Strategy, metaclass=abc.ABCMeta):
    """
    The base class for forecasting strategies
    """

    REQUIRED_CONFIGS = [
        "seed",
        "deterministic"
    ]

    def execute(self, series_name: str, model_factory: ModelFactory) -> Any:
        """
        The primary interface to execute a forecasting strategy

        In this method:

        - Random seeds are set;
        - Target series and corresponding meta-info are prepared;
        - Exceptions are handled;

        :param series_name: The name of a series data to evaluate.
        :param model_factory: A model factory that creates a new model with each invocation.
        :return: The results generated by evaluating a model on a series.
        """
        deterministic_mode = self._get_scalar_config_value("deterministic", series_name)
        seed = self._get_scalar_config_value("seed", series_name)

        if deterministic_mode == "full":
            fix_all_random_seed(seed)
        elif deterministic_mode == "efficient":
            fix_random_seed(seed)

        data_pool = DataPool().get_pool()
        data = data_pool.get_series(series_name)
        meta_info = data_pool.get_series_meta_info(series_name)

        try:
            # single_series_results = self._execute(
            #     data, meta_info, model_factory, series_name
            # )

            # ================================== 开始超参数优化实验 ==================================
            logger.info("***************** [Start Optimize Trial] *****************")
            single_series_results = None  # 评估指标结果
            metrics = self.evaluator.metric_names  # 评估指标名称
            # 设置网格搜索参数
            search_space = {
                'batch_size': [32,64,128,256],
                'lr': [0.0001, 0.0005],
                'num_epochs': [50, 100],
                'patience': [5],
                'seq_len': [96, 336, 512],
                'dropout': [0.2, 0.4, 0.5],
                'd_model': [256,512],
                'd_ff': [512, 1024],
                'n_heads': [1],
                'e_layers': [1],
                'fc_dropout': [0.1]
            }

            # 实验功能主体
            def objective(trial):
                # 定义实验参数
                model_factory.model_hyper_params["batch_size"] = trial.suggest_categorical("batch_size",[32,64,128,256])
                model_factory.model_hyper_params["lr"] = trial.suggest_categorical("lr",[0.0001, 0.0005])
                model_factory.model_hyper_params["num_epochs"] = trial.suggest_categorical("num_epochs",[50, 100])
                model_factory.model_hyper_params["patience"] = trial.suggest_categorical("patience",[5])
                model_factory.model_hyper_params["seq_len"] = trial.suggest_categorical("seq_len",[96, 336, 512])
                model_factory.model_hyper_params["dropout"] = trial.suggest_categorical("dropout",[0.2, 0.4, 0.5])
                model_factory.model_hyper_params["d_model"] = trial.suggest_categorical("d_model", [256,512])
                model_factory.model_hyper_params["d_ff"] = trial.suggest_categorical("d_ff", [512, 1024])
                model_factory.model_hyper_params["n_heads"] = trial.suggest_categorical("n_heads", [1])
                model_factory.model_hyper_params["e_layers"] = trial.suggest_categorical("e_layers", [1])
                model_factory.model_hyper_params["fc_dropout"] = trial.suggest_categorical("fc_dropout", [0.1])
                logger.info("[Trial Number: %d] -> Param:%s", trial.number + 1, model_factory.model_hyper_params)
                # 开始模型计算
                single_series_results = self._execute(
                    data, meta_info, model_factory, series_name
                )
                mapping = metric_mapping(metrics, single_series_results)
                # 定义目标评估
                mse_norm = mapping["mse_norm"]
                mae_norm = mapping["mae_norm"]
                objectives = [mse_norm, mae_norm]
                logger.info("[Trial Number: %d] -> MSE:%s | MAE:%s", trial.number + 1,mse_norm,mae_norm)
                return objectives

            # 创建一个实验：网格搜索、评估方向
            study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space),directions=["minimize", "minimize"])
            # 实验调用
            study.optimize(objective)
            logger.info("[BEST TRIAL]:", study.best_trials)
            logger.info("***************** [End Optimize Trial] *****************")
            # =======================================================================================
        except Exception as e:
            logger.error(e)
            log = f"{traceback.format_exc()}\n{e}"
            single_series_results = self.get_default_result(
                **{FieldNames.LOG_INFO: log}
            )

        return single_series_results

    @abc.abstractmethod
    def _execute(
        self,
        series: pd.DataFrame,
        meta_info: Optional[pd.Series],
        model_factory: ModelFactory,
        series_name: str,
    ) -> Any:
        """
        The execution pipeline of forecasting tasks

        Subclasses are expected to overwrite this method, instead of the :meth:`execute` method.

        :param series: Target series to evaluate.
        :param meta_info: The corresponding meta-info.
        :param model_factory: The factory to create models.
        :param series_name: the name of the target series.
        :return: The evaluation results.
        """

    def _get_eval_scaler(
        self, train_valid_data: pd.DataFrame, train_ratio_in_tv: float
    ) -> Any:
        """
        Gets the scaler used in normalized metrics

        Currently, the scaler is trained on the training series (without the validation data).

        NOTE that this scaler is used only in the metrics, which does not affect model
        training and inferencing.

        :param train_valid_data: The train-validation series.
        :param train_ratio_in_tv: The ratio of the training series when performing train-validation split.
        :return: A scaler object.
        """
        train_data, _ = split_time(
            train_valid_data,
            int(len(train_valid_data) * train_ratio_in_tv),
        )
        scaler = StandardScaler().fit(train_data.values)
        return scaler


# 整合指标名称和值
def metric_mapping(metrics, values):
    # 取较短的长度，以确保一一对应
    min_length = min(len(metrics), len(values))
    # 创建映射字典
    mapping = {metrics[i]: values[i] for i in range(min_length)}
    print("metric_mapping:", mapping)
    return mapping